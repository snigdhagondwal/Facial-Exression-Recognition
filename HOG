tf.reset_default_graph()


facec = cv2.CascadeClassifier('/content/drive/My Drive/Emotion Fera Deep/haarcascade_frontalface_alt.xml')
feature_mat = []
response = []
init = 0

for j in ["Afraid","Anger","Disgusted","Happy","Neutral","sad","Surprise"]:
        files = glob.glob("/content/drive/My Drive/Emotion Fera Deep/Fera2013/"+j+"/*.JPG")
        for i in files:
                fr=cv2.imread(i)
                gray =cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)
                gray = cv2.resize(gray,(128,128))
                faces = facec.detectMultiScale(gray, 1.3, 5)
                for (x, y, w, h) in faces:
                        fc = gray[y:y+h, x:x+w]
                        cv2.rectangle(fr, (x, y), (x + w, y + h), (0, 0, 255), 2)
                #cv2_imshow(fc)
                #cv2.waitKey(10)
                r = cv2.resize(fc,(48,48))     
                fdb,hog_image = hog(r, orientations=8, pixels_per_cell=(8,8),cells_per_block=(4, 4),block_norm= 'L2',visualize=True)
                hog_image = np.float32(hog_image) * 255.0
                #cv2_imshow(hog_image)
                #cv2.waitKey(10)
                feature_mat.append(hog_image)
                response.append(init)
        init=init+1
        
 *** Random Forest***
clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)
X_train, X_test, y_train, y_test = train_test_split(feature_mat, response, test_size=0.33, random_state=42)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_train)
accHOG =accuracy_score(y_pred,y_train)*100
print("HOG Feature Extraction Accuracy")
print(accHOG)

**Resnet**

X, testX, Y, testY = train_test_split(feature_mat , response, test_size=0.33, random_state=42)

X = np.array(X)
testX = np.array(testX)
Y = np.array(Y)
testY = np.array(testY)

Y = tflearn.data_utils.to_categorical(Y, 7)
testY = tflearn.data_utils.to_categorical(testY, 7)

X = X.reshape([-1, 48, 48, 1])
testX = testX.reshape([-1, 48, 48, 1])

img_prep = ImagePreprocessing()
img_prep.add_featurewise_zero_center()
img_prep.add_featurewise_stdnorm()
img_aug = tflearn.ImageAugmentation()
img_aug.add_random_flip_leftright()

# Building Residual Network
n = 5
net = tflearn.input_data(shape=[None, 48, 48, 1], data_preprocessing=img_prep, data_augmentation=img_aug)
net = tflearn.conv_2d(net, nb_filter=16, filter_size=3, regularizer='L2', weight_decay=0.0001)
net = tflearn.residual_block(net, n, 16)
net = tflearn.residual_block(net, 1, 32, downsample=True)
net = tflearn.residual_block(net, n-1, 32)
net = tflearn.residual_block(net, 1, 64, downsample=True)
net = tflearn.residual_block(net, n-1, 64)
net = tflearn.batch_normalization(net)
net = tflearn.activation(net, 'relu')
net = tflearn.global_avg_pool(net)

# Regression
net = tflearn.fully_connected(net, 7, activation='softmax')
mom = tflearn.Momentum(learning_rate=0.1, lr_decay=0.0001, decay_step=32000, staircase=True, momentum=0.9)
net = tflearn.regression(net, optimizer=mom,loss='categorical_crossentropy')

model = tflearn.DNN(net, checkpoint_path='drive/My Drive/Emotion Fera Deep/Model/model_resnet',max_checkpoints=10, tensorboard_verbose=0)
model.fit(X, Y, n_epoch=100, validation_set=(testX, testY),show_metric=True, batch_size=256, run_id='resnet')